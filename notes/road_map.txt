OK, first order of business will need to be to write a STANDARD test fixture.

---
I'm working on some refactoring.
So I ran into this problem that on the one hand, it seems stupid and complicated that every function takes a response.  However, there are two reasons to do it this way:
1) this makes it easy to do list comprehensions over all new files at once.  Class has an extract all function that calls [ImmobilierParser.getX(r) for r in responses] Alternative might be to make, like, a Listing object that does all the actual parsing.  Then start by making an array of new listings, listings =  [Listing(r) for r in responses].  Then call [l.getX() for l in listings].

2) It makes testing much easier because we can simply feed in individual responses.  But, I think this just adds one step?  make fact resposne then call test_listing = Listing(test_response).  test_listing.getX().


So, pros:
  *Avoid a bunch of extra passing around of html files, which is good for performance and possibly good for code readability.
  *Might also help avoid redundant calls-- for example, in Immobilierparser can just get the listing assets when we initialize and store as a class attribute, instead of calling getListingAssets every time.
  * Feels more 'object oriented' to separate out things that handle all of the files from things that search individual files.

Cons:
  *Need to rewrite some things.
  *Feels like it will reduce flexibility because it will be harder to pass different responses to different functions... but as in example above, this can maybe be solved with good classes.
  * Will maybe change inheritance of parser classes?
  * Will end up with more classes to keep track of

Since point of the project was learning anyway, I'll do it.
For this stage of the project, I need to add saving reference number and announcer, duplicate removal script, and finish standardizing/coordinating between spiders and parsers.  Seems like the natural thing to do is add number and announcer to Immobilier, then start refactoring.

Dammit.  Check encoding that I am saving pages too.











DONE
* Made a file class that can update the same raw data file and backfill nas when new features are added, while backing up the old files.  I feel like I have a better understanding of why ppl use databases.
*Wrote some parsing functions
* 95% of parser
* Spider -- Need to write master spider and an inheriting Immobilier spider.  Need to figure out how to select start page.  Need to do a basic parse to getP links and id for all listings, then figure out which listings I do not yet have.  Call Parser on each of them to fill a dataframe with extracted features and save it to a DataFile.

TO DO
Make sure get_old_ids references correct file... make config file?  Use datafile class?
Add file header instead of parsing file name, add link, make class.
write main script
Parser-- fix getAvailability
ReportWriter -- This should be simple, make a yaml file of parameters and parse it to a dataframe filter. Read in the last processed file as data frame and run your filter on it.  Print links column of resulting dataframe to a dated text file.  Maybe reuse some of my file naming/dating code?  Anyway, this part I am pretty confident I can do.
write mapping class --https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude



https://www.immobilier.ch/fr/carte/louer/appartement-maison/page-1?t=rent&c=1;2&p=c11115;c12292;c12211;c10033&nb=false

FindDuplicates -- Need to figure out how to identify when listings on different sites are the same
