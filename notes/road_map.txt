OK, first order of business will need to be to write a STANDARD test fixture.

New architecture:

Listing object is responsible for extracting all the fields from a given page.  The listing class defines getX for every field as a function that just returns the default data not FOUND

Each Listing subclass e.g. ImmobilierListing has its own immobilier_get.yaml file, that defines an input, the expected resut, and the error message for each method.

There is a test_gets_file.py script that checks that all methods of the base class are tested-- this will enforce that every time we write a parser for a new site, we write a test and therefore a function for every field we want to find.  The test_gets_file can also check the yaml test files are correctly formatted, which is sometimes useful.

The ListingParser class is responsible for parsign a group of files, aggregating the data into a dataframe, and then saving the new data.

Considering changing all the gets to return dicts so that Listing Parser can handle each in a standard way?

Still working on modifying and standardizing the get functions for each listing parser (so that they don't take a response)

---
I'm working on some refactoring.
So I ran into this problem that on the one hand, it seems stupid and complicated that every function takes a response.  However, there are two reasons to do it this way:
1) this makes it easy to do list comprehensions over all new files at once.  Class has an extract all function that calls [ImmobilierParser.getX(r) for r in responses] Alternative might be to make, like, a Listing object that does all the actual parsing.  Then start by making an array of new listings, listings =  [Listing(r) for r in responses].  Then call [l.getX() for l in listings].

2) It makes testing much easier because we can simply feed in individual responses.  But, I think this just adds one step?  make fact resposne then call test_listing = Listing(test_response).  test_listing.getX().


So, pros:
  *Avoid a bunch of extra passing around of html files, which is good for performance and possibly good for code readability.
  *Might also help avoid redundant calls-- for example, in Immobilierparser can just get the listing assets when we initialize and store as a class attribute, instead of calling getListingAssets every time.
  * Feels more 'object oriented' to separate out things that handle all of the files from things that search individual files.

Cons:
  *Need to rewrite some things.
  *Feels like it will reduce flexibility because it will be harder to pass different responses to different functions... but as in example above, this can maybe be solved with good classes.
  * Will maybe change inheritance of parser classes?
  * Will end up with more classes to keep track of

Since point of the project was learning anyway, I'll do it.
For this stage of the project, I need to add saving reference number and announcer, duplicate removal script, and finish standardizing/coordinating between spiders and parsers.  Seems like the natural thing to do is add number and announcer to Immobilier, then start refactoring.

Dammit.  Check encoding that I am saving pages too.











DONE
* Made a file class that can update the same raw data file and backfill nas when new features are added, while backing up the old files.  I feel like I have a better understanding of why ppl use databases.
*Wrote some parsing functions
* 95% of parser
* Spider -- Need to write master spider and an inheriting Immobilier spider.  Need to figure out how to select start page.  Need to do a basic parse to getP links and id for all listings, then figure out which listings I do not yet have.  Call Parser on each of them to fill a dataframe with extracted features and save it to a DataFile.

TO DO
Make sure get_old_ids references correct file... make config file?  Use datafile class?
Add file header instead of parsing file name, add link, make class.
write main script
Parser-- fix getAvailability
ReportWriter -- This should be simple, make a yaml file of parameters and parse it to a dataframe filter. Read in the last processed file as data frame and run your filter on it.  Print links column of resulting dataframe to a dated text file.  Maybe reuse some of my file naming/dating code?  Anyway, this part I am pretty confident I can do.
write mapping class --https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude



https://www.immobilier.ch/fr/carte/louer/appartement-maison/page-1?t=rent&c=1;2&p=c11115;c12292;c12211;c10033&nb=false

FindDuplicates -- Need to figure out how to identify when listings on different sites are the same
